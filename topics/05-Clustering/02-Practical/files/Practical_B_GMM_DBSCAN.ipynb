{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae72510a",
      "metadata": {},
      "source": [
        "# Investigating K-Means and Gaussian Mixture Models\n",
        "\n",
        "_Motivation: Expectation-Maximization-Gaussian-Mixtures/EM-for-gmm.ipynb_\n",
        "\n",
        "##  Expectation-Maximisation for Gaussian Mixture Models: optimisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7e9546",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import copy\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "import sys\n",
        "import clusSupport as cs\n",
        "\n",
        "dataDir = \"data\"\n",
        "# Make sure the outputDir subdirectory exists\n",
        "outputDir = \"output/Practical_C_DigitsData\"\n",
        "import os, errno\n",
        "try:\n",
        "    os.makedirs(outputDir)\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_kwds = {'alpha':0.5, 's':25, 'linewidths':0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a067ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "init_means = [\n",
        "    [5, 0], # mean of cluster 1\n",
        "    [1, 1], # mean of cluster 2\n",
        "    [0, 5]  # mean of cluster 3\n",
        "]\n",
        "init_covariances = [\n",
        "    [[.5, 0.], [0, .5]], # covariance of cluster 1\n",
        "    [[.92, .38], [.38, .91]], # covariance of cluster 2\n",
        "    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n",
        "]\n",
        "init_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(4)\n",
        "X = cs.generate_MoG_data(100, init_means, init_covariances, init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c24b639",
      "metadata": {},
      "outputs": [],
      "source": [
        "x0 = []\n",
        "x1 = []\n",
        "for row in X:\n",
        "    x0.append(row[0])\n",
        "    x1.append(row[1])\n",
        "d = np.array([x0, x1])\n",
        "data = d.T\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(data.T[0], data.T[1], c='b', **plot_kwds)\n",
        "frame = plt.gca()\n",
        "frame.axes.get_xaxis().set_visible(False)\n",
        "frame.axes.get_yaxis().set_visible(False)\n",
        "plt.savefig(outputDir + '/threeEllipseBlobs.pdf')\n",
        "\n",
        "#plt.figure()\n",
        "#d = np.vstack(data)\n",
        "#plt.plot(d[:,0], d[:,1],'ko')\n",
        "#plt.rcParams.update({'font.size':16})\n",
        "#plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c8ce19",
      "metadata": {},
      "source": [
        "We can probably pick out the 3 clusters by eye, although there also appear to be some discordant points so the cluster centres are not obvious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2718c62",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(4)\n",
        "\n",
        "# Initialization of parameters\n",
        "\n",
        "# random choice of 3 indices\n",
        "chosen = np.random.choice(len(data), 3, replace=False)\n",
        "# randomly pick 3 of the data points as initial centres\n",
        "initial_means = [data[x] for x in chosen]\n",
        "# Get 3 copies of the covariance of the overall data\n",
        "initial_covs = [np.cov(data, rowvar=0)] * 3\n",
        "# Get 3 copies of 1/3\n",
        "initial_weights = [1/3.] * 3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18331521",
      "metadata": {},
      "outputs": [],
      "source": [
        "#from scipy.stats import multivariate_normal\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#x = np.linspace(0, 5, 10, endpoint=False)\n",
        "#y = multivariate_normal.pdf(x, mean=2, cov=0.5)\n",
        "#plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3153dad",
      "metadata": {},
      "outputs": [],
      "source": [
        "#from scipy.stats import multivariate_normal\n",
        "#var = multivariate_normal(mean=[0,0], cov=[[1,0],[0,1]])\n",
        "#Z = var.pdf([1,0])\n",
        "#var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a731ef6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters after initialization\n",
        "paletteName = 'deep'\n",
        "fontSize = 10\n",
        "cs.plot_contours(data, initial_means, initial_covs, 'Initial clusters', paletteName, fontSize)\n",
        "plt.savefig(outputDir + '/threeEllipseBlobsInitialClusters.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a30aae",
      "metadata": {},
      "source": [
        "As you can see, the initial cluster placements are not particularly good, because two of the initial centres belong to one apparent cluster and a whole set of points lies outside the normal range of their nearest centre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11a1682",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters after running EM to convergence\n",
        "results = cs.EM(data, initial_means, initial_covs, initial_weights)\n",
        "finalWeights = results['weights']\n",
        "finalMeans = results['means']\n",
        "finalCovariances = results['covs']\n",
        "print(init_weights)\n",
        "print(finalWeights)\n",
        "print(init_means)\n",
        "print(finalMeans)\n",
        "print(init_covariances)\n",
        "print(finalCovariances)\n",
        "\n",
        "loglikelihoods = results['loglik']\n",
        "cs.plot_contours(data, results['means'], results['covs'], 'Final clusters', paletteName, fontSize)\n",
        "plt.savefig(outputDir + '/threeEllipseBlobsFinalClusters.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8bce37",
      "metadata": {},
      "source": [
        "As can be seen, the EM algorithm converges after 22 iterations. At that point, the movement of the centres is negligible and the stopping criterion ensures that the algorithm terminates. We can also see from the plot that the EM algorithm has placed the Gaussians so they are centred on the clusters and the covariance contours indicate how the distributions are aligned with the local data in the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b33395",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(range(len(loglikelihoods)), loglikelihoods, linewidth=4)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Log-likelihood')\n",
        "plt.rcParams.update({'font.size':16})\n",
        "plt.tight_layout()\n",
        "plt.savefig(outputDir + '/threeEllipseBlobsObjectiveConvergence.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf7fd30",
      "metadata": {},
      "source": [
        "EM works by minimising the (negative) log likelihood of the data. The plot indicates that the first 3 steps make the most progress, and that subsequent steps just refine the placement until the stopping criterion is met. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e17720b",
      "metadata": {},
      "source": [
        "## Gaussian Mixture Models and stretched data: comparison with K-Means\n",
        "\n",
        "_Motivation: PythonDataScienceHandbook/notebooks/05.12-Gaussian-Mixtures.ipynb_\n",
        "\n",
        "Again, we create some 'blob' data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de57ccb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate some data\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=400, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "X = X[:, ::-1] # flip axes for better plotting\n",
        "\n",
        "plt.figure()\n",
        "plt.axis('equal')\n",
        "plt.scatter(X[:, 0], X[:, 1], s=25, alpha=0.3);\n",
        "plt.savefig(outputDir + '/fourEllipseBlobs.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9a1d42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the data with K Means Labels\n",
        "plot_kwds = {'alpha' : 0.5, 's' : 40, 'linewidths':0}\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "labels = kmeans.fit(X).predict(X)\n",
        "title = \"KMeans fit to 4 blobs\"\n",
        "plt = cs.plot_2dClusters(X, labels, title, paletteName, fontSize, plot_kwds)\n",
        "plt.savefig(outputDir + '/fourEllipseBlobs_Kmeans.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449a5db9",
      "metadata": {},
      "source": [
        "K-Means appears to do a good job assigning points to (colour-coded) clusters. This is not surprising, because the clusters are globular and relatively well separated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d08bc64",
      "metadata": {},
      "outputs": [],
      "source": [
        "cs.plot_kmeans(kmeans, X)\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsKMeansWithDisks.pdf')\n",
        "#from scipy.spatial.distance import cdist\n",
        "#centres = kmeans.cluster_centers_\n",
        "#radii = [cdist(X[labels == i], [centre]).max()\n",
        "#         for i, centre in enumerate(centres)]\n",
        "\n",
        "#fc='#CCCCCC'\n",
        "#plt = cs.overlayDisks(plt, centres, radii, fc, plot_kwds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c3f4ce",
      "metadata": {},
      "source": [
        "The K-means regions are circular and hence a good fit with the data. However, data for clustering does not always have this nice property. We take the data and \"stretch\" it by multiplying by a random matrix, which changes the shape of the clusters as seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cee66ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(13)\n",
        "X_stretched = np.dot(X, rng.randn(2, 2))\n",
        "plt.scatter(X_stretched[:, 0], X_stretched[:, 1], s=25, alpha=0.3);\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsStretched.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0b8a20",
      "metadata": {},
      "source": [
        "We add the K-means circular regions to see how K-means decides on cluster membership."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abb36c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "cs.plot_kmeans(kmeans, X_stretched)\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsStretchedKMeansWithDisks.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0dffd28",
      "metadata": {},
      "source": [
        "K-means still does a good job separting the blue and green clusters, but has difficulty distinguishing between the yellow and purple ones. The circular regions are not a great choice here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5f765b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "gmm = GaussianMixture(n_components=4).fit(X)\n",
        "labels = gmm.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsStretchedGMM.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c1f3de",
      "metadata": {},
      "source": [
        "The Gaussian Mixture Model attempts to place Gaussian distributions so that each is centred on a cluster. Indeed each point is assigned a probability of belonging to each of the Gaussians in the mixture. For many of the points, cluster membership is simple to determine, but the first of the points below could belong to either cluster 1 or cluster 2, but probably to cluster 1, because it has a higher probability of membership with that cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69739a8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "probs = gmm.predict_proba(X)\n",
        "print(probs[:5].round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963c6a5d",
      "metadata": {},
      "source": [
        "If we make the size of the points depend on the cluster membership probability, we can see from the plot below that there are two points at about (2.8,0) and (6.5,0) that have been assigned to the purple cluster rather than either the yellow or the blue cluster, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6c0277",
      "metadata": {},
      "outputs": [],
      "source": [
        "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsGMMProbSize.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec9b6c2",
      "metadata": {},
      "source": [
        "By analogy with the K-Means \"fit\", we can also indicate the Gaussian Mixture regions, see below. Because of sampling reasons, the regions are not quite circular, although they are close."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a595e27",
      "metadata": {},
      "outputs": [],
      "source": [
        "gmm = GaussianMixture(n_components=4, random_state=42)\n",
        "cs.plot_gmm(gmm, X)\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsGMMwithDisks.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ea64bf",
      "metadata": {},
      "source": [
        "We can apply the Gaussian Mixture model to stretched data and it handles it readily, because of the greater flexibility in defining the shape of its regions, see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e5065a",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
        "cs.plot_gmm(gmm, X_stretched)\n",
        "plt.savefig(outputDir + '/fourEllipseBlobsStretchedGMMwithDisks.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e1710b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
