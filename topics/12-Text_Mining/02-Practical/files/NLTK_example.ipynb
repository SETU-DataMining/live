{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "949c9b2d",
      "metadata": {},
      "source": [
        "# Introduction to NLTK\n",
        "\n",
        "This notebook is designed to be used alongside the book _Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit_, written by Steven Bird, Ewan Klein, and Edward Loper of Stanford University, who developed NLTK.\n",
        "\n",
        "This notebook is based on the following sources:\n",
        "\n",
        "- [NLTK book, chapter 1](https://www.nltk.org/book/ch01.html)\n",
        "- [NLTK tutorial](https://www.analyticsvidhya.com/blog/2021/07/nltk-a-beginners-hands-on-guide-to-natural-language-processing/)\n",
        "- [Practice parsing text in NLP with Python](https://opensource.com/article/20/8/intro-python-nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c82f7e4",
      "metadata": {},
      "source": [
        "First install nltk using\n",
        "\n",
        "    conda install -c anaconda nltk\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1e4d64",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daaf629a",
      "metadata": {},
      "source": [
        "You now need to select what corpora (plural of corpus!) and packages you require. For this notebook, it is sufficient to select just the \"book\" identifier. After running this, it is better to comment out the next line so it is not run again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2cebcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25d26d1",
      "metadata": {},
      "source": [
        "## Assign some text and tokenise it\n",
        "\n",
        "First we just check that we can tokenize a short piece of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92c3e94",
      "metadata": {
        "tags": [
          "define_maSpeech"
        ]
      },
      "outputs": [],
      "source": [
        "maSpeech = \"\"\"Friends, Romans, countrymen, lend me your ears;\n",
        "I come to bury Caesar, not to praise him.\n",
        "The evil that men do lives after them;\n",
        "The good is oft interred with their bones;\n",
        "So let it be with Caesar.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c881b768",
      "metadata": {},
      "source": [
        "First we split the text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a5ef0e",
      "metadata": {
        "tags": [
          "sent_tokenize_maSpeech"
        ]
      },
      "outputs": [],
      "source": [
        "print(sent_tokenize(maSpeech))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0138cc0a",
      "metadata": {},
      "source": [
        "Now we split the text into words. Note that punctuation elements (like commas and full stops) are also treated as \"word\" tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28896766",
      "metadata": {
        "tags": [
          "word_tokenize_maSpeech"
        ]
      },
      "outputs": [],
      "source": [
        "print(word_tokenize(maSpeech))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36deef08",
      "metadata": {},
      "source": [
        "## Stage 1: Remove punctuation and make the text lower case\n",
        "\n",
        "We use python's _regular expression_ library `re` to exclude non-alphanumeric characters, such as punctuation, and python's inbuilt `lower()`fnction to convert all alphabetic characters to lower case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c68f59",
      "metadata": {
        "tags": [
          "removePuncLower"
        ]
      },
      "outputs": [],
      "source": [
        "import re\n",
        "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", maSpeech.lower())\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7ba327",
      "metadata": {},
      "source": [
        "Now we can split the lower case text by spaces and we just get a list of those words, without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0459173a",
      "metadata": {
        "tags": [
          "splitMaSpeech"
        ]
      },
      "outputs": [],
      "source": [
        "words = maSpeech.split()\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d088912",
      "metadata": {},
      "source": [
        "## Stage 2: Remove stopwords\n",
        "\n",
        "There are many words in the English language that do not add much information by themselves. As such they can usually be removed, as they are just \"noise\" added to the more useful words that carry most of the meaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da6fd63",
      "metadata": {
        "incorrectly_encoded_metadata": "tags=[removeStopWords\"]"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))\n",
        "\n",
        "# Remove stop words, using a list comprehension\n",
        "filteredWords = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "print(filteredWords)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e796bda",
      "metadata": {},
      "source": [
        "Note that many of the words have been removed, but those that are left are the \"interesting\" ones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13cb435",
      "metadata": {},
      "source": [
        "## Stage 3a: Stemming\n",
        "\n",
        "Some words are essentially similar, e.g., \"friend\" and \"friends\" or \"come\" and \"came\". In NLP terms, each such pair is derived from a \"stem\" word. NLTK offers Porter, Lancaster and Snowball stemmers, which help to standardise the text as follows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9d84e0",
      "metadata": {
        "tags": [
          "applyStemmer"
        ]
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "# Reduce words to their stems\n",
        "porterStemmed = [PorterStemmer().stem(w) for w in filteredWords]\n",
        "print(porterStemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4356f6a1",
      "metadata": {},
      "source": [
        "Note that some word stems are not quite right, e.g., \"buri\" should be \"bury\". The error is because the Porter stemming rule was to remove \"ed\" from the end of the word, where it is found. This rule might work in many cases, but there are exceptions, as here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f24c4d",
      "metadata": {},
      "source": [
        "# Stage 3b: Lemmatisation\n",
        "\n",
        "An alternative approach, that is often more reliable (though the need to lookup a language-specific dictionary can make it run slowly with large amounts of text), is called lemmatisation. A _lemma_ is a candidate root that is obtained by looking up a database of real words. We try this as an alternative standardisation technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568a998e",
      "metadata": {
        "tags": [
          "applyLemmatisation"
        ]
      },
      "outputs": [],
      "source": [
        "#nltk.download('omw-1.4')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# Reduce words to their root form\n",
        "wnLemmed = [WordNetLemmatizer().lemmatize(w) for w in filteredWords]\n",
        "print(wnLemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b7fe37",
      "metadata": {},
      "source": [
        "Note that lemmatisation has generally performed better, but it failed to notice that \"interred\" is the past tense of \"inter\" (to bury), whereas the Porter stemmer correctly applied a rule that transformed \"interred\" to \"inter\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f7aa55",
      "metadata": {},
      "source": [
        "## Next step - analytics\n",
        "\n",
        "Now that we have preprocessed the words from our text, we can generate simple counts, showing how the original set was changed by the 3 stages of preprocessing above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6c2cc4",
      "metadata": {
        "tags": [
          "outputMaSpeechWordSummary"
        ]
      },
      "outputs": [],
      "source": [
        "print(\"Number of words without punctuation {}\".format(len(words)))\n",
        "print(\"Number of words without punctuation and stopwords {}\".format(len(filteredWords)))\n",
        "print(\"Number of such words following stemming {}\".format(len(porterStemmed)))\n",
        "print(\"Number of such words following lemmatisation {}\".format(len(wnLemmed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47962e50",
      "metadata": {},
      "source": [
        "Now we count unique words. Python provides a set datatype, which has the property that its elements are distinct. So adding a list of elements to a set essentially removes repeats, so each element appears only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c682ef",
      "metadata": {
        "tags": [
          "outputMaSpeechDistinctWordSummary"
        ]
      },
      "outputs": [],
      "source": [
        "print(\"Number of distinct words without punctuation {}\".format(len(set(words))))\n",
        "print(\"Number of distinct words without punctuation and stopwords {}\".format(len(set(filteredWords))))\n",
        "print(\"Number of such distinct words following stemming {}\".format(len(set(porterStemmed))))\n",
        "print(\"Number of such distinct words following lemmatisation {}\".format(len(set(wnLemmed))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e26a51",
      "metadata": {},
      "source": [
        "## Loading larger amounts of data\n",
        "\n",
        "We have already installed several collections of text (books, speeches, blogs, posts, newspaper articles, etc.). To show more features of NLTK, we need to load these in memory and to analyse their text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb7ceb3",
      "metadata": {
        "tags": [
          "importCorpora"
        ]
      },
      "outputs": [],
      "source": [
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15496467",
      "metadata": {},
      "source": [
        "`texts()` prints the names and authorship of each of the texts and `sents()` prints the first sentence of each text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbab0888",
      "metadata": {},
      "source": [
        "## Concordance analysis\n",
        "\n",
        "Noting that \"text1\" refers to \"Moby Dick\", we can look at the concordances of a key word like \"monstrous\", by viewing it in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23fa87e",
      "metadata": {
        "tags": [
          "concordance1"
        ]
      },
      "outputs": [],
      "source": [
        "text1.concordance(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ec34a4",
      "metadata": {},
      "source": [
        "It is obvious from these 11 examples that the author (Herman Melville) uses \"monstrous\" in the sense of \"large\", \"imposing\" and \"dangerous\".\n",
        "\n",
        "Famously, the Book of Genesis often refers to long-lived individuals, so concordance analysis can help to select sentences relating to this topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d687b7",
      "metadata": {
        "tags": [
          "concordance3"
        ]
      },
      "outputs": [],
      "source": [
        "text3.concordance(\"lived\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c494790",
      "metadata": {},
      "source": [
        "Each of the characters seemed to have lived for a long time....\n",
        "\n",
        "Returning to \"monstrous\" and _Moby Dick_, it is also informative to see what other words are used by the author in a similar context. This can be done with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8900a02f",
      "metadata": {
        "tags": [
          "similar1"
        ]
      },
      "outputs": [],
      "source": [
        "text1.similar(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b86c0b6b",
      "metadata": {},
      "source": [
        "Given our understanding of language, we can see some of these as near-synonyms of \"monstrous\" as used by the author, although the others seem unrelated to that meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16fcd7e2",
      "metadata": {},
      "source": [
        "Some corpora have a natural temporal ordering. For example, the addresses to Congress are ordered so that more recent speeches are found at the end of the corpus. Consequently, it is interesting to see how the popularity of certain terms has changed over time. Similar temporal patterns might be expected in news reports and social media posts, as words like \"covid\" or \"Kardashian\" rise and fall in importance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585bb8b9",
      "metadata": {
        "tags": [
          "dispersionPlot"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33085f25",
      "metadata": {},
      "source": [
        "It is noticeable that \"democracy\", \"freedom\" and \"America\" are much more common in recent speeches, while \"duties\" has become less common."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b4835f",
      "metadata": {},
      "source": [
        "Returning to the first text (Moby Dick), it is interesting to view the frequency distribution of words.  NLTK provides a function for that purpose. Rather than view all words, we just choose those that occur most frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1869f3",
      "metadata": {
        "tags": [
          "fdist1"
        ]
      },
      "outputs": [],
      "source": [
        "fdist1 = FreqDist(text1)\n",
        "fdist1.most_common(40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de969cb",
      "metadata": {},
      "source": [
        "As you can see, most of the \"words\" are either punctuation or stop words, but interesting words like \"whale\" make it into the list."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e440bb",
      "metadata": {},
      "source": [
        "## Bi-grams and their collocations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b7bdfe",
      "metadata": {},
      "source": [
        "While it is interesting to look at word counts, the difficulty is that words on their own are difficult to interpret. This is the reason why text analysis often considers _bi-grams_, which are pairs of adjacent words, or even _tri-grams_ which are 3 words together in a longer phrase.\n",
        "\n",
        "By choosing to examine the most frequently occurring bigrams, it is much easier to get a sense of the topics of interest in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6220d687",
      "metadata": {
        "tags": [
          "fdist2"
        ]
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "fdist2 = Counter(list(bigrams(text1)))\n",
        "fdist2.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e8600f",
      "metadata": {},
      "source": [
        "Note that combinations of stopwords and punctuation dominate this list of bigrams. As an exercise: you are advised to __preprocess the text (removing punctuation and stopwards and normalising the remaining text) so that the list of bigrams is more \"interesting\"__.\n",
        "\n",
        "A further refinement is to consider only those bigrams that are frequent _and_ this would not be expected from the frequency of the individual words. That is, if two words occur frequently, it is also likely that the associated bigram also appears frequently; this is not particularly interesting. The bigrams of greatest interest might be those that are \"surprising\" because they occur frequently but their individual words are not particularly frequent. Such bigrams are called _collocations_ in the NLP community.\n",
        "\n",
        "Applying this to the speeches made to the US Congress, we see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd66430",
      "metadata": {
        "tags": [
          "collocations4"
        ]
      },
      "outputs": [],
      "source": [
        "text4.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "026107d2",
      "metadata": {},
      "source": [
        "Knowing the preoccupations of US politicians, those two-word phrases should not be surprising. Perhaps the one that stands out is \"Indian tribes\", which probably dates back to the time when the US was expanding westwards and encountered resistance from Native Americans.\n",
        "\n",
        "Another text corpus that reveals interesting/amusing bigrams is text8, which is drawn from personal ads, see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1554428a",
      "metadata": {
        "tags": [
          "collocations8"
        ]
      },
      "outputs": [],
      "source": [
        "text8.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4504c93e",
      "metadata": {},
      "source": [
        "Perhaps it is a pity that we do not have an equivalent corpus of property advertisements, or of wine reviews, each of which has its own \"jargon\" :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ac2fb3",
      "metadata": {},
      "source": [
        "### Collocations applied to \"Pride and Prejudice\"\n",
        "\n",
        "In the following example, we obtain the plain text version of _Pride and Prejudice_ by Jane Austen from Project Gutenberg, to show that it is easy to apply NLTK to text you provide, not just to corpora supplied with NLTK itself.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1f2f04",
      "metadata": {
        "tags": [
          "getPrideAndPrejudice"
        ]
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "# Download text and decode\n",
        "url = \"http://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "text = urllib.request.urlopen(url).read().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aaa00a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.linesep.join(text.split(os.linesep)[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a880d919",
      "metadata": {},
      "source": [
        "First we preprocess the data, in the usual way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3e338d",
      "metadata": {
        "tags": [
          "preprocessP&P"
        ]
      },
      "outputs": [],
      "source": [
        "# Change to lower case and tokenise\n",
        "from nltk.tokenize import word_tokenize\n",
        "lcWords = word_tokenize(text.lower())\n",
        "\n",
        "# Remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('english')\n",
        "filteredWords = [w for w in lcWords if w not in stopWords]\n",
        "\n",
        "# Remove punctuations, including an additional quote types\n",
        "import string\n",
        "punctuationMarks = list(string.punctuation)\n",
        "words = [w for w in filteredWords if (w not in punctuationMarks) and (w not in ('“','”','’'))]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe6cb50",
      "metadata": {},
      "source": [
        "For a change, we will look at the trigrams in the text, and show how to configure the collocation finder to use non-default settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14f2035",
      "metadata": {
        "tags": [
          "trigramsP&P"
        ]
      },
      "outputs": [],
      "source": [
        "# Trigrams\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import TrigramAssocMeasures\n",
        "trigram_collocation = TrigramCollocationFinder.from_words(words)\n",
        "# Top 40 most grequently occurring collocations\n",
        "print(\"Trigrams:\", trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 40))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9cc0bd",
      "metadata": {},
      "source": [
        "There are no prizes for guessing the name of one of the main characters in the novel!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5685a3b",
      "metadata": {},
      "source": [
        "## Parsing\n",
        "\n",
        "So far, we have focused on lexical aspects, from documents to sentences to phrases to words.\n",
        "\n",
        "However text is not just a combination of these elements. Languages have syntax rules that govern how words are put together.  The syntax rules for a language are termed its _grammar_. Each language has its own grammar. Understanding the syntax of the language is a necessary step in being able to derive the meaning of text written in that language.\n",
        "\n",
        "Apart from the language production rules that are essential for understanding the structure of a document, syntax analysis also helps to provide additional, context-specific metadata for each word or phrase. This metadata can be added to the text by means of a process called _part of speech (POS) tagging_.\n",
        "\n",
        "The purpose of parsing is to apply the grammar rules to recognise the structure in each text unit (usually a sentence) and to tag elements of that text with their POS label, as decided by the parser."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c955701",
      "metadata": {},
      "source": [
        "Returning to the opening lines of Mark Antony's speech, we can ask NLTK to tag each word with its POS label. Note that POS tagging is applied to the _original_ text, not to a standardised version because capitalisation, punctuation, plural forms etc. are needed to parse the text correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d163369",
      "metadata": {},
      "outputs": [],
      "source": [
        "words = word_tokenize(maSpeech)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30543515",
      "metadata": {},
      "source": [
        "Using the normal rules of English grammar, we can tag the words with their POS labels. Of course, if a different language was used, say Hindi, the tagger needs to be told to use Hindi grammar instead of English grammar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7200dc0f",
      "metadata": {
        "tags": [
          "posTag"
        ]
      },
      "outputs": [],
      "source": [
        "pos_tagged_text = nltk.pos_tag(words)\n",
        "print(pos_tagged_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94978b0e",
      "metadata": {},
      "source": [
        "The POS codes are [defined by linguists at the University of Pennsylvania](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and are useful for all natural languages. It is convenient to display each word, with its tag and description, using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7c3e66",
      "metadata": {
        "tags": [
          "posTag2"
        ]
      },
      "outputs": [],
      "source": [
        "for pos_tag_word in pos_tagged_text:\n",
        "    print(pos_tag_word[0], \":\")\n",
        "    nltk.help.upenn_tagset(pos_tag_word[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22493090",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Repeat this exercise using other text, especially text where there is structural ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4112cb9f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
