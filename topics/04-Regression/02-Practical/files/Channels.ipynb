{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9769a65a",
      "metadata": {},
      "source": [
        "## Import the relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd45828",
      "metadata": {
        "tags": [
          "imports"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import colours.colorsafe as cs\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d39e4b",
      "metadata": {},
      "source": [
        "## Create the results directory if it does not exist already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14ed0a4",
      "metadata": {
        "tags": [
          "mkResDir"
        ]
      },
      "outputs": [],
      "source": [
        "# See https://www.tutorialspoint.com/How-can-I-create-a-directory-if-it-does-not-exist-using-Python\n",
        "if not os.path.exists('res'):\n",
        "  os.makedirs('res')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46eca46",
      "metadata": {},
      "source": [
        "## Load the data into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d594b150",
      "metadata": {
        "tags": [
          "loadDat"
        ]
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/Advertising.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d3e88a",
      "metadata": {
        "tags": [
          "viewDat"
        ]
      },
      "outputs": [],
      "source": [
        "featureNames = df.columns[:3]\n",
        "targetName = df.columns[3]\n",
        "X3 = df[featureNames]\n",
        "y = df[targetName]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efa480b",
      "metadata": {},
      "source": [
        "## Plot Sales vs. total Advertising spend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4437cb18",
      "metadata": {
        "tags": [
          "setupPlot"
        ]
      },
      "outputs": [],
      "source": [
        "# See https://stackoverflow.com/a/55654661/1988855\n",
        "colours = ['trueBlue', 'orange', 'mustard']\n",
        "colorSet = dict(zip(featureNames, colours))\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel('Advertising [1000 $]')\n",
        "ax.set_ylabel('Sales [Thousands of units]')\n",
        "title=ax.set_title(\"Sales vs. Advertising\", weight=\"bold\")\n",
        "\n",
        "# Plot Sales vs. Advertising spend per channel\n",
        "\n",
        "for featureName in colorSet:\n",
        "  color = cs.ibm5col[colorSet[featureName]]\n",
        "  ax.plot(X3[featureName], y, 'o', color=color, label=featureName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17223c0",
      "metadata": {
        "tags": [
          "plotLegend"
        ]
      },
      "outputs": [],
      "source": [
        "# See https://stackoverflow.com/a/10129461/1988855\n",
        "# ask matplotlib for the plotted objects and their labels\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(lines, labels, loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f5764f",
      "metadata": {
        "tags": [
          "dataPlot"
        ]
      },
      "outputs": [],
      "source": [
        "fig.savefig('res/data.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4df34d6",
      "metadata": {},
      "source": [
        "## Show correlation heatmap for the 3 features and 1 target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5a4255",
      "metadata": {
        "tags": [
          "dataCorrMap"
        ]
      },
      "outputs": [],
      "source": [
        "# See https://seaborn.pydata.org/tutorial/color_palettes.html\n",
        "cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
        "for method in ('pearson', 'kendall', 'spearman'):\n",
        "  mCorr = df.corr(method=method)\n",
        "  hm = sns.heatmap(mCorr, square=True, vmin=0, vmax=1, cmap=cmap)\n",
        "  # See https://stackoverflow.com/a/47765118/1988855\n",
        "  fig = hm.get_figure()\n",
        "  fig.savefig('res/{}CorrHeatmap.pdf'.format(method))\n",
        "  plt.title(f\"{method} correlation (features and target)\")\n",
        "  plt.show()\n",
        "  display(mCorr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77f5f13",
      "metadata": {},
      "source": [
        "## Initialise the model and the choose the score metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f5b0ff",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "initialiseLRandScore"
        ]
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(fit_intercept=True)\n",
        "metric = ('neg_mean_squared_error', 'r2') # see https://scikit-learn.org/1.5/modules/model_evaluation.html#scoring-parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed45530e",
      "metadata": {},
      "source": [
        "## Forward selection 1 - getting the next best feature\n",
        "\n",
        "We identify two operations:\n",
        "\n",
        "1. Finding the loss metric for each of a candidate set of features, and returning the feature with the largest metric\n",
        "2. Accumulate the features found to date, and look for the next best feature.\n",
        "\n",
        "The following Python function `findNextBestFeature()` provides operation 1., and the next block of python provides operation 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefe3ce2",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "findNextBestPred"
        ]
      },
      "outputs": [],
      "source": [
        "def findNextBestFeature(X,foundFeatures):\n",
        "  nP = X.shape[1] # number of columns in X\n",
        "  allFeatures = list(X) # See https://stackoverflow.com/a/19483025\n",
        "  featuresToSearch = set(allFeatures) - set(foundFeatures)\n",
        "  maxScore = -np.inf # can usually do better than this!\n",
        "  for feature in featuresToSearch: # loop over all remaining columns (features) in X\n",
        "    trialFeatures = set(foundFeatures)\n",
        "    trialFeatures.add(feature) # Add this feature to the existing features\n",
        "    #display(trialFeatures)\n",
        "    XcolSubset = X.loc[:,list(trialFeatures)] # all rows and just the trial features\n",
        "    scores = cross_validate(model, XcolSubset, y, cv=5, scoring=metric, return_train_score=True)\n",
        "    #display(scores['test_neg_mean_squared_error'])\n",
        "    score = np.mean(scores['test_neg_mean_squared_error'])\n",
        "    #display(score)\n",
        "    trialFeatures.remove(feature) # remove the current feature from the trialFeatures, to be ready for the next candidate feature\n",
        "    if score > maxScore: # identify the largest score and its associated feature\n",
        "      maxScore = score\n",
        "      metricsForAddedFeature = scores\n",
        "      bestFeatureFound = feature\n",
        "  trialFeatures.add(bestFeatureFound)\n",
        "  print(f\"Selected {bestFeatureFound} so current features are {trialFeatures} with score {score} and the following metrics\")\n",
        "  #display(metricsForAddedFeature)\n",
        "\n",
        "  return maxScore, bestFeatureFound, metricsForAddedFeature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0bf973",
      "metadata": {},
      "source": [
        "## Forward selection 2 - iterating over the search for the next feature to be added\n",
        "\n",
        "Now use `findNextBestFeature` to iterate through the list of features (features)\n",
        "and to keep track of the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a782cb1",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "foundFeatures"
        ]
      },
      "outputs": [],
      "source": [
        "def prioritiseFeatures(X):\n",
        "  nP = X.shape[1]\n",
        "  scoreHistory = dict()\n",
        "  foundFeatures = list()\n",
        "\n",
        "  for i in range(nP): # loop over all columns (features) in X\n",
        "    score, bestFeatureFound, metricsForAddedFeature = findNextBestFeature(X, foundFeatures)\n",
        "    foundFeatures.append(bestFeatureFound)\n",
        "    scoreHistory[bestFeatureFound] = metricsForAddedFeature\n",
        "\n",
        "  #display(foundFeatures)\n",
        "  #display(scoreHistory)\n",
        "  return foundFeatures, scoreHistory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14d81fe",
      "metadata": {},
      "source": [
        "## Preparing a dataframe to make feature analysis easier\n",
        "\n",
        "Parse the scores and map into a dataframe for convenient analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0649b6de",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "parseToDf"
        ]
      },
      "outputs": [],
      "source": [
        "def parseToDF(foundFeatures, scoreHistory):\n",
        "  scoreList = []\n",
        "  for feature in foundFeatures:\n",
        "    scoreDS = scoreHistory[feature]\n",
        "    fitTime = np.mean(scoreDS[\"fit_time\"])\n",
        "    scoreTime = np.mean(scoreDS[\"score_time\"])\n",
        "    testNegMSE = (np.min(scoreDS[\"test_neg_mean_squared_error\"]), np.max(scoreDS[\"test_neg_mean_squared_error\"]))\n",
        "    trainNegMSE = (np.min(scoreDS[\"train_neg_mean_squared_error\"]), np.max(scoreDS[\"train_neg_mean_squared_error\"]))\n",
        "    testR2 = (np.min(scoreDS[\"test_r2\"]), np.max(scoreDS[\"test_r2\"]))\n",
        "    trainR2 = (np.min(scoreDS[\"train_r2\"]), np.max(scoreDS[\"train_r2\"]))\n",
        "    record = {'feature': feature, 'fit_time': fitTime, 'score_time': scoreTime,\n",
        "              'test_neg_mean_squared_error': testNegMSE, 'train_neg_mean_squared_error': trainNegMSE,\n",
        "              'test_r2': testR2, 'train_r2': trainR2}\n",
        "    scoreList.append(record)\n",
        "  \n",
        "  scoresDf = pd.DataFrame.from_dict(scoreList)\n",
        "  return scoresDf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17083b73",
      "metadata": {},
      "source": [
        "## Derive the scores dataframe for a given feature matrix X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46433164",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "deriveScoresDf"
        ]
      },
      "outputs": [],
      "source": [
        "def deriveScoresDf(X):\n",
        "  foundFeatures, scoreHistory = prioritiseFeatures(X)\n",
        "  scoresDf = parseToDF(foundFeatures, scoreHistory)\n",
        "  return scoresDf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cc6c05",
      "metadata": {
        "tags": [
          "prepScoresDf3"
        ]
      },
      "outputs": [],
      "source": [
        "scoresDf3 = deriveScoresDf(X3)\n",
        "display(scoresDf3[[\"feature\", \"test_neg_mean_squared_error\", \"test_r2\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d1ba2f",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "\n",
        "Note that there is a lot of overlap between the test scores for `TV + Radio` and `TV + Radio + Newspaper`\n",
        "(the minimum and maximum for each class of metric are very similar). Hence, `Newspaper` adds almost\n",
        "no value to the model, so we do not include this feature.\n",
        "\n",
        "## Interaction term\n",
        "\n",
        "To add an interaction term, we need to multiply the two features in that interaction. However, this can\n",
        "introduce scaling difficulties, so we scale the features so that all features (including the new\n",
        "interaction term) will have the same scaling.\n",
        "Doing the scaling after multiplication makes it easier to undo the scaling afterwards, if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81333af7",
      "metadata": {
        "tags": [
          "prepareX4"
        ]
      },
      "outputs": [],
      "source": [
        "X4 = X3.copy()\n",
        "X4[\"TV:Radio\"] = X4['TV'] * X4['Radio']\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X4)\n",
        "X4scaled = scaler.transform(X4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be3f6bbb",
      "metadata": {},
      "source": [
        "## Generate the scores for X4 (with the interaction term)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f474ce9",
      "metadata": {
        "tags": [
          "prepScoresDf4"
        ]
      },
      "outputs": [],
      "source": [
        "scoresDf4 = deriveScoresDf(X4)\n",
        "display(scoresDf4[[\"feature\", \"test_neg_mean_squared_error\", \"test_r2\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3769326",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "\n",
        "The `TV:Radio` feature is a more significant addition to the model than `Radio` on its own.\n",
        "Using the same criterion as before, we can surmise that the best model is `TV + TV:Radio`.\n",
        "\n",
        "## Exercise: Reproduce this notebook using `statsmodels` instead of `scikit-learn`."
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
